# ğŸš€ ELT Data Platform (Production-Ready)

This project demonstrates the design of a **modern ELT data pipeline** that is scalable, secure, and production-ready.  
It covers the complete lifecycle of data: from ingestion and storage, to transformation, quality validation, lineage tracking, observability, and serving to BI tools.  

---

## ğŸ¯ Objective
Build a **reliable and auditable ELT platform** to collect, store, transform, validate, monitor, and serve data for analytics and business operations.

---

## ğŸ— High-Level Architecture

1. **Sources** â†’ Databases, APIs, CSVs, Event Streams (Kafka/Kinesis).  
2. **Ingestion** â†’ Airbyte / Fivetran (CDC-enabled) for Extract + Load.  
3. **Landing (Raw Storage)** â†’ Object Storage (S3 / ADLS / GCS).  
4. **Warehouse (ELT Target)** â†’ Snowflake / BigQuery / Redshift.  
5. **Transformation (dbt)** â†’ Staging â†’ Intermediate â†’ Marts.  
6. **Data Quality (Great Expectations + dbt tests)** â†’ Validation of data contracts and business rules.  
7. **Lineage & Catalog (OpenLineage + Marquez / DataHub)** â†’ Full data lineage and metadata management.  
8. **Storage Catalog / Docs (dbt docs)** â†’ Auto-generated documentation and model lineage.  
9. **Serving & BI** â†’ Looker / Power BI / Superset.  
10. **Observability** â†’ CloudWatch / Prometheus + Grafana (logs, metrics, alerts).  
11. **Orchestration** â†’ Airflow / Dagster to manage DAGs, retries, SLAs.

---

## ğŸ” Security

- **IAM / RBAC**: Principle of Least Privilege, Service Accounts (no hardcoded credentials).  
- **Encryption**:  
  - At Rest: S3 KMS, Snowflake TDE.  
  - In Transit: TLS everywhere.  
- **Data Governance**:  
  - Row-Level Security & Masking for PII.  
  - Audit logs in Warehouse.  
  - Lineage registered in DataHub/Marquez.  

---

## âœ… Data Quality

- **dbt tests** â†’ Unique, Not Null, Relationships.  
- **Great Expectations** â†’ Advanced checks: distributions, regex, ranges.  
- **Data Contracts** â†’ Enforced schema agreements with source teams.  
- **Fail Fast** â†’ Critical test failures block downstream publish.  
- **Data Docs** â†’ Auto-generated reports for transparency.

---

## ğŸ“Š Observability & Logging

- **Pipeline Monitoring**:  
  - Airflow task metrics (success/failure, SLA, duration).  
  - SLA alerts if DAG exceeds defined thresholds.  
- **Data Monitoring**:  
  - Freshness of key tables.  
  - Volume anomalies.  
  - Schema drift detection.  
- **Logs**:  
  - Airflow â†’ ELK / CloudWatch.  
  - Ingestion / dbt logs persisted and centralized.  

---

## â± Orchestration

- **Airflow / Dagster** to orchestrate the entire pipeline.  
- **Dependency Management**: Ingestion â†’ Landing â†’ Warehouse â†’ dbt â†’ GE â†’ Publish.  
- **Retries**: 3 retries with exponential backoff.  
- **SLA**: Critical DAGs complete by 6AM.  
- **Triggers**: Event-based scheduling supported.  
- **OpenLineage Integration**: Each operator emits lineage metadata.

---

## ğŸ—‚ Documentation & Catalog

- **dbt docs** â†’ Auto-generated model documentation + internal lineage graph.  
- **DataHub / Amundsen** â†’ Metadata, catalog, ownership tracking.  
- **Lineage Graph** â†’ From raw source to BI dashboards.  

---

## ğŸ’¾ Backup & Disaster Recovery

- **Storage** â†’ S3/ADLS versioning + lifecycle policies.  
- **Warehouse** â†’ Snowflake Time Travel, BigQuery snapshots.  
- **Airflow** â†’ RDS snapshot & HA deployment.  
- **Recovery Testing** â†’ Regular DR drills.  

---

## ğŸš¨ Incident Response

- **Alert Channels** â†’ Slack, PagerDuty, Email.  
- **Alert Types**:  
  - Pipeline failure (Airflow DAG/task).  
  - Data quality test failure.  
  - Freshness/volume anomalies.  
  - Unauthorized access (security).  
- **Runbooks** â†’ Step-by-step guides for issue resolution.  

---

## ğŸ“ˆ SLAs & SLOs

- **Pipeline Success Rate**: â‰¥ 99%  
- **Data Freshness**: â‰¤ 15 minutes delay  
- **Data Quality**: â‰¥ 99.5% of critical tests passing  
- **MTTD (Mean Time to Detect)**: < 5 minutes  
- **MTTR (Mean Time to Recover)**: < 30 minutes  

---

## ğŸ›  Development Workflow (CI/CD)

- **GitOps** â†’ All changes tracked in Git.  
- **CI**:  
  - Lint + compile dbt models.  
  - Run unit tests (pytest for utilities).  
  - `dbt build --select state:modified`.  
- **CD**:  
  - Deploy Airflow DAGs + dbt models to staging â†’ prod.  
- **Feature Branching**: All changes via PRs.  
- **Rollback**: Blue/green deployments or version pinning.  

---

## ğŸ”¢ ELT Steps (End-to-End Flow)

1. **Sources** â†’ Raw data from DBs/APIs/CSV/Events.  
2. **Ingestion** (Airbyte/Fivetran with CDC).  
3. **Landing** â†’ Raw storage in S3/ADLS/GCS.  
4. **Warehouse** â†’ Load raw into Snowflake/BigQuery/Redshift.  
5. **Transformations** â†’ dbt models build staging â†’ marts.  
6. **Data Quality (GE + dbt tests)** â†’ Validate data quality, fail = stop + alert.  
7. **Lineage & Catalog (OpenLineage)** â†’ Track lineage + metadata.  
8. **Docs (dbt docs)** â†’ Auto-generated documentation.  
9. **Serving & BI** â†’ Clean data to Looker/Power BI/Superset.  
10. **Observability** â†’ Metrics, logs, alerts in CloudWatch/Grafana.  
11. **Orchestration** â†’ Airflow/Dagster manages full cycle.  

ğŸ“Œ Order of execution:  
`1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ 6 (GE) â†’ 7 (OpenLineage) â†’ 8/9/10 â†’ managed by 11 (Airflow)`  


